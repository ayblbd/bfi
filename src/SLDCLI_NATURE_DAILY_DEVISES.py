from pyspark.sql import SparkSession
from pyspark.sql.functions import expr, substring, udf, when, col, current_date, lit
import pyspark.sql.functions as F
from pyspark.sql.types import *
import subprocess

spark = SparkSession.builder.appName("dim sldcli").getOrCreate()

# Example of loading data using spark.read.format()
df_dm_sldcli = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/nifi/DLKPROD/DM_SLDCLI")
df_rp_gl_bct = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/nifi/DLKPROD/RP_GL_BCT/RP_GL_BCT")
df_reference_nature_compte_pci = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/nifi/DLKPROD/REFERENCE_NATURE_COMPTE_PCI/REFERENCE_NATURE_COMPTE_PCI")
df_cours_devise = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/nifi/DLKPROD/TAUX_CHANGE_BAM_HIST")
df_exploit_ref_devise = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/nifi/ODS/EXPLOIT_REF_DEVISE/EXPLOIT_REF_DEVISE")

df_dm_sldcli.createOrReplaceTempView("DM_SLDCLI")
df_rp_gl_bct.createOrReplaceTempView("RP_GL_BCT")
df_reference_nature_compte_pci.createOrReplaceTempView("REFERENCE_NATURE_COMPTE_PCI")
df_cours_devise.createOrReplaceTempView("TAUX_CHANGE_BAM_HIST")
df_exploit_ref_devise.createOrReplaceTempView("EXPLOIT_REF_DEVISE")

result_df = spark.sql(
    """
    WITH devises as (
SELECT a.CODE_DEVISE, b.CD_DEV_OPER, a.TAUXDECHANGE, a.TYPE, a.DATE FROM TAUX_CHANGE_BAM_HIST a
join EXPLOIT_REF_DEVISE b 
on a.CODE_DEVISE=b.CD_DEVISE)
SELECT DISTINCT tmp1.DTTRTM, tmp1.PCI, tmp1.NATURE_EXTERNE, tmp1.NUMERODECOMPTE
, tmp1.DEVISE, tmp2.TYPE, CAST(tmp1.SOLDEJOURCR AS DECIMAL(38,4)) as SOLDEJOURCR,
CAST(tmp1.SOLDEJOURCR AS DECIMAL(38,4)) * CAST(tmp2.TAUXDECHANGE AS DECIMAL(38,4)) as SOLDEJOURCR_devise, 
CAST(tmp1.SOLDEJOURDB AS DECIMAL(38,4)) as SOLDEJOURDB,CAST(tmp1.SOLDEJOURDB AS DECIMAL(38,4)) * CAST(tmp2.TAUXDECHANGE AS DECIMAL(38,4)) as SOLDEJOURDB_devise from (SELECT c.DTTRTM, f.PCI, 
    (case
    WHEN d.nature_externe is null then SUBSTRING(c.NUMERODECOMPTE, 4, 3)
    else d.nature_externe
    end ) as NATURE_EXTERNE, c.NUMERODECOMPTE, c.DEVISE, c.SOLDEJOURCR, c.SOLDEJOURDB,
    ( CASE
        WHEN f.PCI IN ('111300' ,'111400', '112110', '112120', '461100', '462100' ,'466130') THEN 'BILLET'
        ELSE 'VIREMENT'
    END) AS TYPE
from DM_SLDCLI c left join 
RP_GL_BCT d on c.NUMERODECOMPTE=d.COMPTE_CLIENT left join 
REFERENCE_NATURE_COMPTE_PCI f ON
c.NTCPTE=f.Nature) tmp1
left join devises tmp2 
on tmp1.DEVISE=tmp2.CD_DEV_OPER and tmp1.TYPE=tmp2.TYPE and CAST(tmp1.DTTRTM AS DATE)=CAST(tmp2.DATE AS DATE);
"""
)


# Definition of the domain and table for saving the result-----------------6 530,154
COUCHE = "SOCLE"
DOMAINE = "CDM_COMPTABILITE"
TABLE = "SLDCLI_NATURE_DAILY_DEVISES"

# Saving the resulting table
result_df.coalesce(1).write.parquet(
    f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP", mode="overwrite")

# Searching for .parquet files in the temporary folder
parquet_files = subprocess.run(["hdfs", "dfs", "-find", 
                f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP/*.parquet"],
                capture_output=True, text=True)

# Handling the existence of .parquet files
if parquet_files.returncode == 0 and parquet_files.stdout:
    # Remove the _SUCCESS file generated by Spark
    subprocess.run(["hdfs", "dfs", "-rm", f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP/_SUCCESS"])
# Handling the existence of .parquet files
   # subprocess.run(["hdfs", "dfs", "-rm", "-r", f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}])

    # Rename the part-XXXXX.parquet files to the target table name
    for filename in parquet_files.stdout.splitlines():
        subprocess.run(["hdfs", "dfs", "-mv", filename, 
            f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}"])
    
    # Remove the temporary folder after renaming the files
    subprocess.run(["hdfs", "dfs", "-rm", "-r", f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP"])

# Stop the Spark session if no further operations are needed
spark.stop()