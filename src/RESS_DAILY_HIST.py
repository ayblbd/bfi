from pyspark.sql import SparkSession
from pyspark.sql.functions import expr, substring, udf, when, col, current_date, lit, concat, expr
import pyspark.sql.functions as F
from pyspark.sql.types import *
import subprocess

spark = SparkSession.builder.appName("APP RESS_EMP").getOrCreate()

#df_a = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/SOCLE/CDM_COMPTABILITE/SLDCLI_NATURE_DAY_19_04")
df_a = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/SOCLE/CDM_COMPTABILITE/SLDCLI_NATURE_DAILY_DEVISES")
df_c = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/nifi/DLKPROD/MAPPING_TYPE_RESSOURCE/MAPPING_TYPE_RESSOURCE")
df_e = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/SOCLE/COMPTABILITE/TYPE_RESS_EMP")


result_df = df_a.alias("a") \
    .join(df_c.alias("c"),
          expr("a.PCI = concat(c.PCI, '0') AND a.NATURE_EXTERNE = c.NATURE"),
          "left_outer") \
    .join(df_e.alias("e"),
          expr("a.PCI = concat(e.PCI, '0') AND a.NATURE_EXTERNE = e.NATURE and e.CATEGORIE='RESS'"),
          "left_outer") \
    .select(col("a.DTTRTM"), col("a.PCI"), col("a.NATURE_EXTERNE").alias("NATURE"), col("a.numerodecompte"), col("a.devise")
            , col("a.soldejourcr"), col("a.soldejourdb"), col("a.soldejourcr_devise"), col("a.soldejourdb_devise"),
            col("c.TYPE_RESSOURCE").alias("NOUVEAU_TYPE"), 
            col("e.Type_RESS_EMPO").alias("ANCIEN_TYPE")).filter(df_a["soldejourcr"].isNotNull())

# Definition of the domain and table for saving the result-----------------6 530,154
COUCHE = "SOCLE"
DOMAINE = "CDM_COMPTABILITE"
TABLE = "RESS_DAILY_HIST"

# Saving the resulting table
result_df.coalesce(1).write.parquet(
    f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP", mode="overwrite")

# Searching for .parquet files in the temporary folder
parquet_files = subprocess.run(["hdfs", "dfs", "-find", 
                f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP/*.parquet"],
                capture_output=True, text=True)

# Handling the existence of .parquet files
if parquet_files.returncode == 0 and parquet_files.stdout:
    # Remove the _SUCCESS file generated by Spark
    subprocess.run(["hdfs", "dfs", "-rm", f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP/_SUCCESS"])
    
# Handling the existence of .parquet files
   # subprocess.run(["hdfs", "dfs", "-rm", "-r", f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}])

    # Rename the part-XXXXX.parquet files to the target table name
    for filename in parquet_files.stdout.splitlines():
        subprocess.run(["hdfs", "dfs", "-mv", filename, 
            f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}"])
    
    # Remove the temporary folder after renaming the files
    subprocess.run(["hdfs", "dfs", "-rm", "-r", f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP"])

# Stop the Spark session if no further operations are needed
spark.stop()