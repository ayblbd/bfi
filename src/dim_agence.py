from pyspark.sql import SparkSession
from pyspark.sql.functions import expr, substring, udf, when, col, current_date, lit
import pyspark.sql.functions as F
from pyspark.sql.types import *
import subprocess

# Ensure a SparkSession is created and available as `spark`
spark = SparkSession.builder.appName("DIM AGENCE").getOrCreate()

# Load your data from a Parquet file
ods_exp_t608_2 = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/nifi/ODS/EXP_T608_2")
df_niv1 = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/SOCLE/AGENCE/ENTITE_NIVEAU_1")
df_niv0 = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/SOCLE/AGENCE/ENTITE_NIVEAU_0")
df_niv3 = spark.read.format("parquet").load("hdfs://10.9.125.134:8020/SOCLE/AGENCE/ENTITE_NIVEAU_3")


ods_exp_t608_2.createOrReplaceTempView("EXP_T608_2")
df_niv1.createOrReplaceTempView("ENTITE_NIVEAU_1")
df_niv0.createOrReplaceTempView("ENTITE_NIVEAU_0")
df_niv3.createOrReplaceTempView("ENTITE_NIVEAU_3")


result_df = spark.sql( """
WITH niv_1 as (
    select     
    ENTITE as CODEENTITE,
    NOMENTIT as NOMENTITE,
    DIRZONE AS CODEBANQUE,
    GRREG AS CODEREGION,
    LOCALI,
    CVILLE,
    SECTGEO,
    OPERATIONNELLE
from EXP_T608_2 where (ENTITE is not null and GRREG is not null and DFVAL>CURRENT_DATE() and SUCCURSALE is null and ENTITE=GRREG) 
union ALL
select CODEENTITE, NOMENTITE, CODEBANQUE, CODEREGION, LOCALI, CVILLE, SECTGEO,
    OPERATIONNELLE  from ENTITE_NIVEAU_1
)
SELECT tmp2.*, d.NOMBANQUE from ( 
    SELECT tmp1.*, c.NOMENTIT as NOMREGION from (SELECT (CASE
    when CODEAGENCE IS NULL then CODESUCC
    else CODEAGENCE
    end) as CODEAGENCE,
    (CASE 
    WHEN NOMAGENCE IS NULL THEN NOMSUCC
    else NOMAGENCE
    end) as NOMAGENCE,
    (CASE 
    WHEN CODEAGENCE IS NULL THEN NULL
    else CODESUCC
    end) as CODESUCC,
    (CASE 
    WHEN NOMAGENCE IS NULL THEN NULL
    else NOMSUCC
    end) as NOMSUCC,
    (CASE 
    WHEN CODEREGION0 IS NULL THEN CODEREGION1
    else CODEREGION0
    end) as CODEREGION,
    (CASE 
    WHEN CODEBANQUE0 IS NULL THEN CODEBANQUE1
    else CODEBANQUE0
    end) as CODEBANQUE
from ( 
select a.CODEAGENCE,a.NOMAGENCE, a.CODEREGION as CODEREGION0, b.CODEREGION as CODEREGION1, b.CODEENTITE as CODESUCC, b.NOMENTITE as NOMSUCC, a.CODEBANQUE as CODEBANQUE0, b.CODEBANQUE as CODEBANQUE1 from ENTITE_NIVEAU_0 a 
RIGHT join niv_1 b
on a.CODESUCCURSALE=b.CODEENTITE)) tmp1 
left join (select     
    ENTITE ,
    NOMENTIT,
    DIRZONE AS CODEBANQUE,
    LOCALI,
    CVILLE,
    SECTGEO,
    OPERATIONNELLE
from EXP_T608_2 WHERE (GRREG IS NULL or GRREG=ENTITE or GRREG not in (select CODEREGION from ENTITE_NIVEAU_0) or GRREG not in (select CODEREGION from niv_1)) AND (DFVAL > CURRENT_DATE()) AND (DIRZONE IS NOT NULL or ENTITE in (select CODEBANQUE from ENTITE_NIVEAU_3))
) c 
on tmp1.CODEREGION=c.ENTITE
) tmp2 
left join ENTITE_NIVEAU_3 d 
on tmp2.CODEBANQUE=d.CODEBANQUE

""")

# Definition of the domain and table for saving the result
COUCHE = "SOCLE"
DOMAINE = "AGENCE"
TABLE = "NIVREGROUP"

# Saving the resulting table
result_df.coalesce(1).write.parquet(
    f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP", mode="overwrite")

# Searching for .parquet files in the temporary folder
parquet_files = subprocess.run(["hdfs", "dfs", "-find", 
                f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP/*.parquet"],
                capture_output=True, text=True)

# Handling the existence of .parquet files
if parquet_files.returncode == 0 and parquet_files.stdout:
    # Remove the _SUCCESS file generated by Spark
    subprocess.run(["hdfs", "dfs", "-rm", f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP/_SUCCESS"])
# Handling the existence of .parquet files
   # subprocess.run(["hdfs", "dfs", "-rm", "-r", f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}])

    # Rename the part-XXXXX.parquet files to the target table name
    for filename in parquet_files.stdout.splitlines():
        subprocess.run(["hdfs", "dfs", "-mv", filename, 
            f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}"])
    
    # Remove the temporary folder after renaming the files
    subprocess.run(["hdfs", "dfs", "-rm", "-r", f"hdfs://10.9.125.134:8020/{COUCHE}/{DOMAINE}/{TABLE}_TEMP"])

# Stop the Spark session if no further operations are needed
spark.stop()
